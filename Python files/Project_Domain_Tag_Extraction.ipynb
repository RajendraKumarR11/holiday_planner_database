{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nCONCLUSIONS:\\n\\n\\nThis code reads data taken from an external source \\'Kaggle\\', performs data cleaning, data reformat, stores it in a dataframe and \\nfinally exports it to a CSV file\\nCONTRIBUTIONS:\\n\\nRAJENDRA KUMAR RAJKUMAR - 75% \\nMONISH  HIRISAVE RAGHU - 25%\\n\\nCITATIONS:\\n\\n1. https://www.geeksforgeeks.org\\n2. https://github.com/nikbearbrown/INFO_6210\\n3. Reddit developer docs\\n\\nThe code with regard to extraction of information from reddit was used from the above mentioned resources\\n\\nOriginal writtten code - 90%\\nCode referenced from external sources(but modified suiting needs) - 10%\\n\\nLICENSE:\\n\\nCopyright <2019> <RAJENDRA KUMAR RAJKUMAR, MONISH  HIRISAVE RAGHU>\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tourist Attraction cleaning of data\n",
    "# This code \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import json\n",
    "import re\n",
    "from re import sub\n",
    "from pandas.io.json import json_normalize\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "\n",
    "# Twitter/Reddit/Instagram data from respective csv files 'City_Twitter_Extracted_Tweet.csv', \n",
    "# 'City_Reddit_Extracted_RedditPost.csv','Instagram_hashtag_latest.csv' and being stored under respective dataframes\n",
    "tweet_input_df = pd.read_csv('City_Twitter_Extracted_Tweet.csv')\n",
    "reddit_input_df = pd.read_csv('City_Reddit_Extracted_RedditPost.csv')\n",
    "instagram_input_df = pd.read_csv('Instagram_hashtag_latest.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Twitter\n",
    "#Initialisting tweet_dict dictionary\n",
    "tweet_dict = dict()\n",
    "from collections import Counter\n",
    "# Tweet content of all tweets are concatenated into a single string\n",
    "all_tweet_data = ' '.join(tweet_input_df[\"Content\"])\n",
    "\n",
    "# All tweet content data is split into individual words\n",
    "split_it = all_tweet_data.split() \n",
    "#Frequency of occurence of every word is counted\n",
    "tweet_data= Counter(split_it) \n",
    "# Iterating over the most commony used words and filtering out words with length\n",
    "# greater than or equal to 4 and frequency greater than 1 and updating tweet_dict\n",
    "for key, value in tweet_data.most_common():\n",
    "    if(len(key) >= 4 and value > 1):\n",
    "        tweet_dict.update({key:value})\n",
    "        \n",
    "# Creating a list with common words used in Twitter with less significanc, which can be ignored\n",
    "twitter_ignore_list = ['with', 'never', 'that', 'were', 'just', '2019', 'this', 'great', 'been', 'from', 'I’ll', 'thank', 'where', 'they', '@TheAdamsEra:', 'forget', 'those', 'five', 'teams', 'passed', 'wanna', 'you!', 'supposed', '@AdamSchefter:', 'Drafts', 'tremendous', 'hosts.', 'There', 'scene', 'lik…', 'seen', 'over', 'these', 'have', 'Please', 'their', 'home', 'morning', '&amp;', 'last', 'April', 'miss', 'talk', 'ain’t', 'pick', 'make', 'Draft', 'Harbor', 'looking',  'High', 'having', 'only', 'students', 'heading', 'This', 'left', 'Heading', 'Great', 'guys', '@SonyaRosee:', 'RETWEET.', 'friend', 'MISSING.', 'Sunday', '21st', 'near', 'Embas…', 'upcoming', 'Southeast,', 'world', 'like', 'team', 'some', 'calling', 'draft', '@WhiteHouse:', 'President', '@realDonaldTrump', '@FLOTUS', 'traveled', 'yesterday', 'Drug', 'Abuse', 'Heroin', 'Summit,', 'con…', 'your', 'wonderful', 'Best', 'Charleston', 'getting', 'school', 'Nature', '@BrianBrinkleyOK:', '#Sooners', 'Notre', 'Dame', 'most', 'overall', 'picks', 'five:', 'Selmon', 'Your', 'Nashville.', 'cause', 'People', 'I’ve', 'across', 'Harbor,', '@Bucknuts247:', '#49ers', '@nbsmallerbear', '#NFLDraft', '(FREE)', 'https://t.co/naYSYfAQd8', 'https://t.co/bc6zowD6Tc', '@BR_NBA:', 'Meanwhile', 'https://t.co/CE81vQvbLS']\n",
    "\n",
    "# Filtering tweet_dict, by removing items from twitter_ignore_list\n",
    "final_twitter_dict = dict()\n",
    "for key, value in tweet_dict.items():\n",
    "    if key not in twitter_ignore_list:\n",
    "        final_twitter_dict.update({key:value})\n",
    "\n",
    "# Creating two separate lists with twitter_keys(Tag name) and twitter_values (Respective frequency)\n",
    "final_twitter_keys_list =list(final_twitter_dict.keys())\n",
    "final_twitter_values_list =list(final_twitter_dict.values())\n",
    "\n",
    "\n",
    "\n",
    "# Reddit\n",
    "#Initialisting reddit_dict dictionary\n",
    "reddit_dict = dict()\n",
    "from collections import Counter\n",
    "# Reddit post content of all reddit posts are concatenated into a single string\n",
    "all_reddit_data = ' '.join(reddit_input_df[\"Content_Title\"])\n",
    "\n",
    "# All tweet reddit post data is split into individual words\n",
    "reddit_split_it = all_reddit_data.split()\n",
    "#Frequency of occurence of every word is counted\n",
    "reddit_data= Counter(reddit_split_it) \n",
    "# Iterating over the most commony used words and filtering out words with length\n",
    "# greater than or equal to 4 and frequency greater than 1 and updating reddit_dict\n",
    "for key, value in reddit_data.most_common():\n",
    "    if(len(key) >= 3 and value > 1):\n",
    "        reddit_dict.update({key:value})\n",
    "        \n",
    "# Creating a list with common words used in Reddit with less significanc, which can be ignored\n",
    "reddit_ignore_list = ['for', 'the', 'trip', 'and', 'road', 'from', 'Traveling', 'New', 'USA', 'can', 'Looking', 'need', 'there', 'through', 'take', 'what', 'some', 'Visiting', 'you', 'this', 'What', 'your', 'York', 'was', 'out', 'that', 'cheapest', 'advice', 'international', 'Long', 'Should', 'days', 'travel', 'best', 'California?', 'Help', 'looking', 'Las', 'Vegas', 'suggestions', 'then', 'June.', 'Planning']\n",
    "\n",
    "# Filtering reddit_dict, by removing items from reddit_ignore_list\n",
    "final_reddit_dict = dict()\n",
    "for key, value in reddit_dict.items():\n",
    "    if key not in reddit_ignore_list:\n",
    "        final_reddit_dict.update({key:value})\n",
    "# Creating two separate lists with reddit_keys(Tag name) and reddit_values (Respective frequency)\n",
    "final_reddit_keys_list =list(final_reddit_dict.keys())\n",
    "final_reddit_values_list =list(final_reddit_dict.values())\n",
    "\n",
    "\n",
    "\n",
    "# Instagram\n",
    "instagram_input_df['Hash Tag']= instagram_input_df['Hash Tag'].astype(str)\n",
    "#Initialisting instagram_dict dictionary\n",
    "instagram_dict = dict()\n",
    "# Instagram tag content content of all reddit posts are concatenated into a single string\n",
    "from collections import Counter\n",
    "all_instagram_data = ' '.join(instagram_input_df[\"Hash Tag\"])\n",
    " \n",
    " # All instagram post data is split into individual words\n",
    "instagram_split_it = all_instagram_data.split() \n",
    "instagram_data= Counter(instagram_split_it) \n",
    "# Iterating over the most commony used words and filtering out words with length\n",
    "# greater than or equal to 3 and frequency greater than 1 and updating reddit_dict\n",
    "for key, value in instagram_data.most_common():\n",
    "    if(len(key) >= 3 and value > 1):\n",
    "        instagram_dict.update({key:value})\n",
    "        \n",
    "# Creating a list with common words used in Instagram with less significance, which can be ignored\n",
    "instagram_ignore_list = ['love', 'usa', 'nan',  'nyc', 'travel', 'nature', 'music', 'photooftheday', 'instagram', 'southbeach', 'sedona', 'natchez', 'anchorage', 'india', 'sexylipnails', 'winterparknailtech', 'maitlandnails', 'altamontesprings', 'nailsart', 'nailsofinstagram', 'brooklyn', 'paris', 'washington', 'barharbor', 'atl', 'arizona', 'saudiarabia', 'southcarolina', 'africa', 'america', 'europe', 'instagood', 'fashion', 'vegas', 'beautiful', 'newjersey', 'philly', 'nebraska', 'canada', 'pakistan', 'life', 'orlandonails', 'louisiana', 'australia', 'asia', 'wanderlust', 'vacation', 'maine', 'hongkong', 'italy', 'northcarolina', 'nigeria', 'rnb', 'happy', 'instadaily', 'hollywood', 'bachelorparty', 'sydney', 'travelphotography', 'alaska', 'acadianationalpark', 'orangecounty', 'finland', 'cyprus', 'south', 'kenya', 'nails', 'nailsoftheday', 'nailsonfleek', 'nailsnailsnails', 'nailswagg', 'nails4today', 'manicure', 'newyorktimes', 'colorado', 'alabama', 'design', 'lifequotes', 'musicvideo', 'explore', 'hiphop', 'nfldraft', 'lit', 'memphis', 'sunset', 'france', 'austintexas', 'fees', 'oregon', '2019', 'radio', 'mississippi', 'hiking', 'ghana', 'miamiheat', 'cleveland', 'precisamenteencartagena', 'cholon', 'baru', 'playablanca', 'bachelorette', 'boats', 'ohio', 'new', 'fashionblogger', 'selfie', 'followforfollowback', 'nightlifenyc', 'nycnightlife', 'vip', 'upscale', 'slowjams', 'blackandwhite', 'romance', 'soulnightevents', 'grownandsexy', 'mancrush', 'womancrush', 'single', 'couple', 'couplesgoals', 'manhattan', 'photographer', 'cash', 'followme', 'youtube', 'tbt', 'style', 'ocean', 'microblading', 'tokyo', 'spotify', 'singer', 'complex', 'film', 'santamonica', 'beauty', 'blogger', 'portrait', 'mood', 'hypebae', 'dubai', 'culture', 'artist', 'nakidmagazine', 'dazed', 'inkedgirls', 'freshlyinked', 'tattoolife', 'uniquestyle', 'inkedmagazine', 'alternativefashion', 'ravewear', 'inkedfashion', 'vogueitalia', 'bodypositive', 'curvyinkedbabe', 'coachellafest', 'palmsprings', 'desertphotoshoot', 'modelsearch', 'killercurves', 'redhair', 'redhairbombshell', 'turnt', 'cmg', 'migos', 'actionpackap', 'djhardhitta1', 'djlife', 'optoutside', 'thursday', 'amazing', 'travelgram', 'instatravel', 'travelholic', 'architecturephotography', 'wisconsin', 'westvirginia', 'sanjose', 'amazon', 'minnesota', 'montana', 'kansascity', 'explorepage', 'lifestyle', 'lashextensions', 'lashes', 'trending', 'worldwide', 'eagles', 'vacay', 'metal', 'punk', 'creative', 'ootd', 'spring', 'graffiti', 'stl', 'ハワイ', 'singapore', 'philippines', 'santodomingo', 'atx', 'friends', 'visitmaine', 'tachira', 'táchira', 'sancristobal', 'ureña', 'tariba', 'colon', 'coloncito', 'cordero', 'bogotá', 'rubio', 'cucuta', 'cúcuta', 'venezuela', 'sanjosecito', 'bogota', 'palmira', 'gocho', 'publicidad', 'noticia', 'informacion', 'internacionales', 'ultimahora', 'últimahora', 'lafria', 'frontera', 'venezolanosenelexterior', 'gochosenelexterior', 'lagrita', 'cali', 'producer', 'smoke', 'writer', 'naturelovers', 'austintx', 'germany', 'unitedstates', 'kitchendesign', 'arusha', 'ccm', 'mwanza', 'basketball', 'ballislife', 'sports', 'nba', 'nbaplayoffs', 'mumbai', 'entrepreneurlife', 'designer', 'fashionista', 'dancer', 'rapper', 'peace', 'party', 'dance', 'housemusic', 'djing', 'pioneer', 'valentino', 'winterparkeyelashextensions', 'eyelashes', 'nailartvideos', 'spagiftcard', 'newyorknails', 'beverlyhillsnails', 'womensupportingwomen', 'apresgelx', 'orlandonailtech', 'photo', 'hair', 'money', 'lottery', 'beautifull', 'decor', 'lifestlye', 'interiordesign', 'property', 'propertymanagement', 'vlone', 'haggle', 'vlog', 'youtuber', 'tranquility', 'rich', 'development', 'commercialrealestate', 'homedecor', 'hotel', 'media', 'passion', 'movie', 'rap', 'friday', 'quotes', 'repost', 'dogsofinstagram', 'gym', 'antalya', 'istanbul', 'follow', 'soundcloud', '90s', 'swv', 'applemusic', 'songwriter', 'studio', 'blackgirlmagic', 'wshh', 'worldstar', 'lilyachty', 'actor', 'picoftheday', 'pictureoftheday', 'portraitphotography', 'fitness', 'home', 'exclusive', 'streetwear', 'streetfashion', 'hypebeast', 'highsnobiety', 'online', 'shop', 'freshlydippedorgnls', 'freshlydipped', 'boutiquelife', 'haveafreshday', 'visualarts', 'entertainment', 'blessed', 'happiness', 'family', 'luxury', 'classic', 'smile', 'clouds', 'sun', 'pinksky', 'streetphotography', 'igtravel', 'passionpassport', 'sincity', 'seekthesimplicity', 'logodesign', 'likeforlikes', 'vancouver', 'independent', 'legal', 'legit', 'safe', 'invest', 'investment', 'process', 'processing', 'trust', 'try', 'fast', 'quick', 'cashinminutes', 'moneyneedstobemade', 'states', 'simple', 'getpaidtoday', 'instafashion', 'businesswoman', 'westcoast', 'eastcoast', 'barber', 'denverbarber', 'barberlife', 'quebarbaro', 'barbersindenver', 'barbaros', 'coloradobarber', 'barbarians', 'barbadenver', 'milehighbarber', 'straightrazor', 'fresh', 'barba', 'hairstylesformen', 'bringingsexyback', 'menscut', 'barbergang', 'denverbarbers', 'wecanupgradeyou', 'newkidsontheblock', 'yoga', 'outfit', 'lashlift', 'lash', 'lashliftandtint', 'lashobsessed', 'eyelashextensions', 'eyelashlove', 'denverlashes', 'milehighlashes', 'lashesdenver', 'eyelashextensionsdenver', 'microbladingdenver', 'hennabrowsdenver', 'brows', 'coloradolashes', 'volumelashes', 'hybridvolumelashes', 'megavolunelashes', 'mega', 'advancedlashartist', 'graphics', 'mixtape', 'graphicdesign', 'breaktheinternet', 'bearfacechallenge', 'bavelli', 'dallascowboys', 'nino', 'thriller', 'attention', 'biggest', 'powerful', 'newjackcity', 'djs', 'strippers', 'dancers', 'rappers', 'actors', 'dmv', 'wolves', 'independentartist', 'triangleparts', 'melanin', 'philadelphiaeagles', 'flyeaglesfly', 'trusttheprocess', 'mywestelm', 'roomstyling', 'stayhere', 'cozy', 'eaglesfootball', 'lgbtq', 'travelagent', 'marierodriguez', 'pacificnorthwest', 'rock', 'procreate', 'mariners', 'reflection', 'pikeplacemarket', 'holiday', 'seattledogs', 'viatgersdc', 'ig_catalonia', 'ig_catalunya', 'ig_spain_', 'ig_usa_', 'batonrouge', 'santafe', 'newmexico', 'diamonddoesdiamond', 'writersofinstagram', 'nola', 'traveladdict', 'tourism', 'adventure', 'traveltheworld', 'travelblog', 'popart', 'blues', 'avengersendgame', 'gorgeous', 'pretty', 'jeromearizona', 'tgif', 'outdoorlover', 'naturephotography', 'beautifuldestinations', 'waikiki', 'luckywelivehawaii', 'hawaiifood', 'hawaiifoodie', 'aloha', 'lips', 'ホノルル', 'oahu', 'ripcity', 'pdx', 'hardcore', 'portlandoregon', 'pnw', 'san',  'food', 'texascapitol', 'video', 'yahoofood', 'nofilter', 'throwback', 'natchezmississippi', 'fsx', 'fsxpix', 'fsx_pictures', 'fsx_pics', 'fsxvideos', 'fsxpics', 'flightsimulator', 'flightsimulatorx', 'captainsim', 'c130', 'hercules', 'c130hercules', 'lockheed', 'uscoastguard', 'aerosoft', 'panc', 'traveling', 'instaphoto', 'fanpage']\n",
    "\n",
    "# Filtering reddit_dict, by removing items from instagram_ignore_list\n",
    "final_instagram_dict = dict()\n",
    "for key, value in instagram_dict.items():\n",
    "    if key not in instagram_ignore_list:\n",
    "        final_instagram_dict.update({key:value})\n",
    "# Creating two separate lists with instagram_keys(Tag name) and instagram_key_values (Respective frequency)\n",
    "final_instagram_keys_list =list(final_instagram_dict.keys())\n",
    "final_instagram_values_list =list(final_instagram_dict.values())     \n",
    "\n",
    "\n",
    "\n",
    "final_twitter_df = pd.DataFrame(data = final_twitter_keys_list,columns = ['Tag_Name'])\n",
    "final_twitter_df['Tag_Occurence_Count'] = final_twitter_values_list  \n",
    "\n",
    "final_reddit_df = pd.DataFrame(data = final_reddit_keys_list,columns = ['Tag_Name'])\n",
    "final_reddit_df['Tag_Occurence_Count'] = final_reddit_values_list \n",
    "\n",
    "final_instagram_df = pd.DataFrame(data = final_instagram_keys_list,columns = ['Tag_Name'])\n",
    "final_instagram_df['Tag_Occurence_Count'] = final_instagram_values_list \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exporting the data from all dataframes into respective csvs\n",
    "final_twitter_df.to_csv(\"Twitter_Extracted_Doman_Tags.csv\", index=False)\n",
    "final_reddit_df.to_csv(\"Reddit_Extracted_Doman_Tags.csv\", index=False)\n",
    "final_instagram_df.to_csv(\"Instagram_Extracted_Doman_Tags.csv\", index=False)\n",
    "\n",
    "\"\"\"\"\n",
    "CONCLUSIONS:\n",
    "\n",
    "\n",
    "This code reads data taken from an external source, clean the same as per the project requirement and exports it to a csv file\n",
    "\n",
    "CONTRIBUTIONS:\n",
    "\n",
    "RAJENDRA KUMAR RAJKUMAR - 50% \n",
    "MONISH  HIRISAVE RAGHU - 50%\n",
    "\n",
    "CITATIONS:\n",
    "\n",
    "1. https://www.geeksforgeeks.org\n",
    "2. https://github.com/nikbearbrown/INFO_6210\n",
    "3. Reddit developer docs\n",
    "\n",
    "The code with regard to extraction of information from reddit was used from the above mentioned resources\n",
    "\n",
    "Original writtten code - 90%\n",
    "Code referenced from external sources(but modified suiting needs) - 10%\n",
    "\n",
    "LICENSE:\n",
    "\n",
    "Copyright <2019> <RAJENDRA KUMAR RAJKUMAR, MONISH  HIRISAVE RAGHU>\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
